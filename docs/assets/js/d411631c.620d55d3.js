"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[101],{1270:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var o=n(5893),a=n(1151);const r={title:"GPU Acceleration"},s=void 0,i={id:"gpu",title:"GPU Acceleration",description:"At this time, only NVIDIA GPU acceleration is supported. Please open an issue if you'd like to see support for other GPU vendors.",source:"@site/docs/gpu.md",sourceDirName:".",slug:"/gpu",permalink:"/aikit/gpu",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/gpu.md",tags:[],version:"current",frontMatter:{title:"GPU Acceleration"},sidebar:"sidebar",previous:{title:"API Specifications",permalink:"/aikit/specs"},next:{title:"Kubernetes Deployment",permalink:"/aikit/kubernetes"}},l={},c=[{value:"NVIDIA",id:"nvidia",level:2}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",p:"p",pre:"pre",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.admonition,{type:"note",children:(0,o.jsx)(t.p,{children:"At this time, only NVIDIA GPU acceleration is supported. Please open an issue if you'd like to see support for other GPU vendors."})}),"\n",(0,o.jsx)(t.h2,{id:"nvidia",children:"NVIDIA"}),"\n",(0,o.jsxs)(t.p,{children:["AIKit supports GPU accelerated inferencing with ",(0,o.jsx)(t.a,{href:"https://github.com/NVIDIA/nvidia-container-toolkit",children:"NVIDIA Container Toolkit"}),". You must also have ",(0,o.jsx)(t.a,{href:"https://www.nvidia.com/en-us/drivers/unix/",children:"NVIDIA Drivers"})," installed on your host machine."]}),"\n",(0,o.jsxs)(t.p,{children:["For Kubernetes, ",(0,o.jsx)(t.a,{href:"https://github.com/NVIDIA/gpu-operator",children:"NVIDIA GPU Operator"})," provides a streamlined way to install the NVIDIA drivers and container toolkit to configure your cluster to use GPUs."]}),"\n",(0,o.jsxs)(t.p,{children:["To get started with GPU-accelerated inferencing, make sure to set the following in your ",(0,o.jsx)(t.code,{children:"aikitfile"})," and build your model."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-yaml",children:"runtime: cuda         # use NVIDIA CUDA runtime\n"})}),"\n",(0,o.jsxs)(t.p,{children:["For ",(0,o.jsx)(t.code,{children:"llama"})," backend, set the following in your ",(0,o.jsx)(t.code,{children:"config"}),":"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-yaml",children:"f16: true             # use float16 precision\ngpu_layers: 35        # number of layers to offload to GPU\nlow_vram: true        # for devices with low VRAM\n"})}),"\n",(0,o.jsx)(t.admonition,{type:"tip",children:(0,o.jsx)(t.p,{children:"Make sure to customize these values based on your model and GPU specs."})}),"\n",(0,o.jsx)(t.admonition,{type:"note",children:(0,o.jsxs)(t.p,{children:["For ",(0,o.jsx)(t.code,{children:"exllama"})," and ",(0,o.jsx)(t.code,{children:"exllama2"})," backends, GPU acceleration is enabled by default and cannot be disabled."]})}),"\n",(0,o.jsxs)(t.p,{children:["After building the model, you can run it with ",(0,o.jsx)(t.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html#gpu-enumeration",children:(0,o.jsx)(t.code,{children:"--gpus all"})})," flag to enable GPU support:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:'# for pre-made models, replace "my-model" with the image name\ndocker run --rm --gpus all -p 8080:8080 my-model\n'})}),"\n",(0,o.jsx)(t.p,{children:"If GPU acceleration is working, you'll see output that is similar to following in the debug logs:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:"5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr ggml_init_cublas: found 1 CUDA devices:\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr   Device 0: Tesla T4, compute capability 7.5\n...\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: using CUDA for GPU acceleration\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: mem required  =   70.41 MB (+ 2048.00 MB per state)\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: offloading 32 repeating layers to GPU\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: offloading non-repeating layers to GPU\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: offloading v cache to GPU\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: offloading k cache to GPU\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: offloaded 35/35 layers to GPU\n5:32AM DBG GRPC(llama-2-7b-chat.Q4_K_M.gguf-127.0.0.1:43735): stderr llm_load_tensors: VRAM used: 5869 MB\n"})})]})}function u(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>i,a:()=>s});var o=n(7294);const a={},r=o.createContext(a);function s(e){const t=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(r.Provider,{value:t},e.children)}}}]);